{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "martial-figure",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/nalinjindal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import snowball\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "loose-grace",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('Scraping reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sonic-evaluation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\n  Please do not buy expensive product like...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n  Bought the mobile from appario retail lt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n  Awesome Phone. Nice upgrade from iPhone ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\n  My Phone is Producing Too Much Heat Even...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\n  This is a big scam. I received the iphon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews\n",
       "0  \\n\\n  Please do not buy expensive product like...\n",
       "1  \\n\\n  Bought the mobile from appario retail lt...\n",
       "2  \\n\\n  Awesome Phone. Nice upgrade from iPhone ...\n",
       "3  \\n\\n  My Phone is Producing Too Much Heat Even...\n",
       "4  \\n\\n  This is a big scam. I received the iphon..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "numeric-measurement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22308, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "covered-commissioner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'more', 'were', 'your', 'are', 'ain', 'her', 'does', 'wasn', 'those', \"won't\", 'myself', 'between', 'below', \"needn't\", 'few', 'the', 'again', \"you've\", 'where', 'any', 'mightn', 'in', 'if', 'shouldn', 'do', 'hers', 'what', 'both', \"mightn't\", 'these', 'nor', 'me', 'so', 'it', \"couldn't\", 'off', 'too', 'or', 'into', 't', 'to', 'up', 'its', 'while', 'how', 'wouldn', 'this', 'she', 'doing', \"that'll\", 'which', 'out', 'for', 'such', 'his', 'once', 'no', 'whom', 'has', 'until', 'very', 'been', 'on', 'am', 'should', 'he', 'of', 'from', \"doesn't\", \"shouldn't\", \"haven't\", 'before', \"aren't\", 'than', 'ours', 'yourself', 'themselves', 'because', 'same', 'most', 'll', 'at', 'itself', 'by', 'there', 've', 'own', 'our', \"she's\", 'now', 'their', 'that', 'then', 'against', 'is', \"weren't\", 'hadn', 'some', 'not', 'was', 'down', 'and', 'ourselves', 'aren', 'i', 'hasn', 'each', 'isn', 'they', 'why', 'haven', 'during', 'you', 'them', 'after', 'be', \"shan't\", 'above', \"hasn't\", 'theirs', 'o', 'yours', 'but', \"hadn't\", 'being', 'won', 'him', 're', 'needn', \"isn't\", 'will', 'herself', 'with', 'y', \"you'll\", 'can', 'don', 'have', 'here', 'd', 'did', \"you'd\", 'weren', \"you're\", \"it's\", 'a', 'an', 'through', 'himself', \"wasn't\", \"mustn't\", 'we', 'as', 'over', 'further', 'didn', 'only', 'shan', 'other', 'yourselves', 'had', 'all', 'just', \"didn't\", 'couldn', 'mustn', 'who', \"don't\", 'about', 'doesn', 'having', 's', \"should've\", 'under', 'ma', 'my', 'm', \"wouldn't\", 'when'}\n"
     ]
    }
   ],
   "source": [
    "stop=set(stopwords.words('english'))\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "atlantic-drill",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import numpy\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "stop=set(stopwords.words('english'))\n",
    "snow = nltk.stem.SnowballStemmer('english')\n",
    "stop\n",
    "def decontracted(phrase):\n",
    "    phrase=re.sub(r\"won't\",\"will not\",phrase)\n",
    "    phrase=re.sub(r\"can't\",\"can not\",phrase)\n",
    "    phrase=re.sub(r\"n\\'t\",\"not\",phrase)\n",
    "    phrase=re.sub(r\"\\'re\",\"are\",phrase)\n",
    "    phrase=re.sub(r\"\\'s\",\"is\",phrase)\n",
    "    phrase=re.sub(r\"\\'d\",\"would\",phrase)\n",
    "    phrase=re.sub(r\"\\'ll\",\"will\",phrase)    \n",
    "    phrase=re.sub(r\"\\'t\",\"not\",sentence)\n",
    "    phrase=re.sub(r\"\\'ve\",\"have\",sentence)\n",
    "    phrase=re.sub(r\"\\'m\",\"am\",sentence)\n",
    "    return phrase\n",
    "preprocessed_reviews=[]\n",
    "for sentence in data['reviews'].values:\n",
    "    sentence=re.sub(r\"http\\S+\",\" \",sentence)\n",
    "    sentence=BeautifulSoup(sentence,'lxml').get_text()\n",
    "    cleanr=re.compile('<.*?>')\n",
    "    sentence=re.sub(cleanr,' ',sentence)\n",
    "    sentence=decontracted(sentence)\n",
    "    sentence=re.sub(\"\\S\\*\\d\\S*\",\" \",sentence)\n",
    "    sentence=re.sub(\"[^A-Za-z]+\",\" \",sentence)\n",
    "    sentence=re.sub(r'[?|!|\\'|\"|#]',r' ',sentence)\n",
    "    sentence=re.sub(r'[.|,|)|(|\\|/]',r' ',sentence)\n",
    "    sentence='  '.join(snow.stem(e.lower()) for e in sentence.split() if e.lower() not in stop)\n",
    "    preprocessed_reviews.append(sentence.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "monetary-taxation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'review  day  usedisplay  upgard  iphon  ole  display  realli  good  fhd  sunlight  struggl  camera  although  camera  notic  big  differ  daylight  somewhat  differ  low  light  nd  camera  ultrawideselfi  camera  realli  good  video  dolbi  vision  hdr  good  limit  ur  devic  work  social  media  post  instagram  batteri  attach  screen  shot  percentprovid  hr  screen  time  compact  light  weightus  samsung  w  chargerfrom  percent  taken  minut  back  side  glass  get  hot  charg  even  use  youtub  use  camera  super  hot  need  solv  issueif  iphon  wait  next  gennot  worthi  pay  extra  ole  displayupd  use  week'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_reviews[2000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "indoor-glucose",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7369)\t1\n",
      "  (0, 1414)\t1\n",
      "  (0, 7672)\t1\n",
      "  (0, 310)\t1\n",
      "  (0, 5029)\t1\n",
      "  (0, 10071)\t1\n",
      "  (0, 352)\t1\n",
      "  (0, 1253)\t1\n",
      "  (0, 6132)\t2\n",
      "  (0, 458)\t1\n",
      "  (0, 8384)\t1\n",
      "  (0, 5712)\t1\n",
      "  (0, 4459)\t1\n",
      "  (0, 11008)\t1\n",
      "  (0, 8760)\t1\n",
      "  (0, 270)\t1\n",
      "  (0, 1481)\t1\n",
      "  (0, 6164)\t2\n",
      "  (0, 11042)\t2\n",
      "  (0, 7751)\t2\n",
      "  (0, 7017)\t1\n",
      "  (0, 7464)\t1\n",
      "  (0, 9399)\t1\n",
      "  (0, 5538)\t1\n",
      "  (0, 6228)\t1\n",
      "  :\t:\n",
      "  (0, 1642)\t2\n",
      "  (0, 6431)\t1\n",
      "  (0, 5612)\t1\n",
      "  (0, 5298)\t1\n",
      "  (0, 7336)\t1\n",
      "  (0, 4758)\t1\n",
      "  (0, 9509)\t1\n",
      "  (0, 6335)\t1\n",
      "  (0, 4058)\t1\n",
      "  (0, 2214)\t1\n",
      "  (0, 433)\t1\n",
      "  (0, 8799)\t3\n",
      "  (0, 4172)\t1\n",
      "  (0, 3596)\t2\n",
      "  (0, 6601)\t1\n",
      "  (0, 2033)\t1\n",
      "  (0, 3213)\t1\n",
      "  (0, 8798)\t1\n",
      "  (0, 2480)\t1\n",
      "  (0, 9978)\t1\n",
      "  (0, 2949)\t1\n",
      "  (0, 2430)\t1\n",
      "  (0, 7964)\t1\n",
      "  (0, 8554)\t1\n",
      "  (0, 8236)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count=CountVectorizer()\n",
    "Reviews_BOW=count.fit_transform(preprocessed_reviews)\n",
    "print(Reviews_BOW[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "criminal-battlefield",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 95905)\t1\n",
      "  (0, 17146)\t1\n",
      "  (0, 100233)\t1\n",
      "  (0, 3784)\t1\n",
      "  (0, 64877)\t1\n",
      "  (0, 131264)\t1\n",
      "  (0, 4580)\t1\n",
      "  (0, 64891)\t1\n",
      "  (0, 15007)\t1\n",
      "  (0, 78310)\t2\n",
      "  (0, 6229)\t1\n",
      "  (0, 109658)\t1\n",
      "  (0, 73352)\t1\n",
      "  (0, 57856)\t1\n",
      "  (0, 141883)\t1\n",
      "  (0, 114329)\t1\n",
      "  (0, 2518)\t1\n",
      "  (0, 18522)\t1\n",
      "  (0, 79040)\t2\n",
      "  (0, 142367)\t2\n",
      "  (0, 101201)\t2\n",
      "  (0, 90531)\t1\n",
      "  (0, 97232)\t1\n",
      "  (0, 123206)\t1\n",
      "  (0, 69736)\t1\n",
      "  :\t:\n",
      "  (0, 4640)\t1\n",
      "  (0, 124680)\t1\n",
      "  (0, 81394)\t1\n",
      "  (0, 52403)\t1\n",
      "  (0, 28449)\t1\n",
      "  (0, 17168)\t1\n",
      "  (0, 5551)\t1\n",
      "  (0, 115480)\t1\n",
      "  (0, 54260)\t1\n",
      "  (0, 46137)\t1\n",
      "  (0, 85284)\t1\n",
      "  (0, 26345)\t1\n",
      "  (0, 115465)\t1\n",
      "  (0, 40553)\t1\n",
      "  (0, 115373)\t1\n",
      "  (0, 46085)\t1\n",
      "  (0, 31967)\t1\n",
      "  (0, 100809)\t1\n",
      "  (0, 130034)\t1\n",
      "  (0, 115458)\t1\n",
      "  (0, 37027)\t1\n",
      "  (0, 31339)\t1\n",
      "  (0, 103979)\t1\n",
      "  (0, 96008)\t1\n",
      "  (0, 111242)\t1\n"
     ]
    }
   ],
   "source": [
    "count=CountVectorizer(ngram_range=(1,2))\n",
    "Bigram_Counts=count.fit_transform(preprocessed_reviews)\n",
    "print(Bigram_Counts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "lesbian-buffalo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8236)\t0.10295346778206306\n",
      "  (0, 8554)\t0.12224910615084508\n",
      "  (0, 7964)\t0.17347057500832713\n",
      "  (0, 2430)\t0.09756462820805249\n",
      "  (0, 2949)\t0.18934713103016002\n",
      "  (0, 9978)\t0.16861448120050876\n",
      "  (0, 2480)\t0.09649592458038232\n",
      "  (0, 8798)\t0.1036266076723118\n",
      "  (0, 3213)\t0.08807943255610823\n",
      "  (0, 2033)\t0.10737789501407495\n",
      "  (0, 6601)\t0.15693912463065546\n",
      "  (0, 3596)\t0.17760786382472546\n",
      "  (0, 4172)\t0.10414665390444479\n",
      "  (0, 8799)\t0.3012508053254889\n",
      "  (0, 433)\t0.10121058370769626\n",
      "  (0, 2214)\t0.09170857471370565\n",
      "  (0, 4058)\t0.059657196448686736\n",
      "  (0, 6335)\t0.06596631399311852\n",
      "  (0, 9509)\t0.15465711255034964\n",
      "  (0, 4758)\t0.1348172187114122\n",
      "  (0, 7336)\t0.10146515380545444\n",
      "  (0, 5298)\t0.15693912463065546\n",
      "  (0, 5612)\t0.12804193753192006\n",
      "  (0, 6431)\t0.15326785646075763\n",
      "  (0, 1642)\t0.20044663210776428\n",
      "  :\t:\n",
      "  (0, 6228)\t0.16355395694101893\n",
      "  (0, 5538)\t0.0713789588731273\n",
      "  (0, 9399)\t0.14601875985492216\n",
      "  (0, 7464)\t0.11943482801814527\n",
      "  (0, 7017)\t0.12328195954638199\n",
      "  (0, 7751)\t0.18241963829424473\n",
      "  (0, 11042)\t0.11461383783825567\n",
      "  (0, 6164)\t0.1547519720331022\n",
      "  (0, 1481)\t0.0370465385785632\n",
      "  (0, 270)\t0.056319569199397915\n",
      "  (0, 8760)\t0.09376711369022334\n",
      "  (0, 11008)\t0.09057340687292567\n",
      "  (0, 4459)\t0.06999046945438522\n",
      "  (0, 5712)\t0.141808440046539\n",
      "  (0, 8384)\t0.12502028139187707\n",
      "  (0, 458)\t0.12389685393567888\n",
      "  (0, 6132)\t0.11488920611159843\n",
      "  (0, 1253)\t0.07408256818906561\n",
      "  (0, 352)\t0.1096296910745386\n",
      "  (0, 10071)\t0.11305651039672064\n",
      "  (0, 5029)\t0.061919172225437795\n",
      "  (0, 310)\t0.06036999978798061\n",
      "  (0, 7672)\t0.05376662009243478\n",
      "  (0, 1414)\t0.05514594513413834\n",
      "  (0, 7369)\t0.0825385559302232\n"
     ]
    }
   ],
   "source": [
    "counts=TfidfVectorizer()\n",
    "cnt=counts.fit_transform(preprocessed_reviews)\n",
    "print(cnt[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "complimentary-catch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22308\n",
      "[ 0.69368812  0.1622408  -0.59031525 -0.90485629 -0.1154097  -0.5145687\n",
      "  0.53241318  0.0790426   0.50757988 -0.5650794   0.40510166  0.56400139\n",
      " -0.50566089  0.48620667  1.01879176  0.06015156  0.36439264  0.34805341\n",
      " -0.35292916 -0.1834911   0.25177754 -0.02507679 -0.15384624 -0.31606117\n",
      "  0.20743035  0.52276433 -0.52780759  0.12334925  0.18841665 -0.66858048\n",
      "  0.17346755 -0.29938129 -0.44317801  0.22454291 -0.41922926 -0.01927845\n",
      "  0.22109071  0.26122026  0.66530588  0.40144325 -0.15836493  0.08951257\n",
      "  0.48360597 -0.96757811  0.41676002 -0.0320585  -0.32141502 -0.2234151\n",
      "  0.47199943  0.01836816]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "list_of_sentences=[]\n",
    "for sentence in preprocessed_reviews:\n",
    "    list_of_sentences.append(sentence.split())\n",
    "w2v_model=Word2Vec(list_of_sentences,min_count=5,size=50,workers=4)\n",
    "w2v_words=list(w2v_model.wv.vocab)\n",
    "sent_vectors=[]\n",
    "for sent in list_of_sentences:\n",
    "    sent_vec=np.zeros(50)\n",
    "    cnt_words=0\n",
    "    for word in sent:\n",
    "        if word in w2v_words:\n",
    "            vec=w2v_model.wv[word]\n",
    "            sent_vec=sent_vec+vec\n",
    "            cnt_words=cnt_words+1\n",
    "    if cnt_words!=0:\n",
    "        sent_vec=sent_vec/cnt_words\n",
    "    sent_vectors.append(sent_vec)\n",
    "print(len(sent_vectors))   \n",
    "print(sent_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sufficient-crawford",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('Preprocessing_1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-sierra",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
